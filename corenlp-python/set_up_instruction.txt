Set-up Instructions:

Prerequites:
0. you might need the following: (linux)
sudo apt-get install python-dev
sudo apt-get install libmysqlclient-dev

1. install mysql.

2. install git: https://help.github.com/articles/set-up-git

3  Install nltk:
Install Setuptools: http://pypi.python.org/pypi/setuptools
Install Pip: run sudo easy_install pip
Install Numpy (optional): run sudo pip install -U numpy
Install PyYAML and NLTK: run sudo pip install -U pyyaml nltk
Test installation: run python then type import nltk

3.5 download wordnet for nltk corpora and punkt from nltk models using nltk.download

4.install MySQLdb (interface between Mysql and python API): pip install MySQL-python

5. install jsonrpclib and corenlp-python:
sudo pip install pexpect unidecode jsonrpclib   
git clone https://bitbucket.org/torotoki/corenlp-python.git
	  cd corenlp-python
wget http://nlp.stanford.edu/software/stanford-corenlp-full-2013-06-20.zip
unzip stanford-corenlp-full-2013-06-20.zip

6. install scikit-learn: scikit-learn.org/stable/install.html

7. also make sure you have numpy, scipy and matplotlib. a useful link: http://penandpants.com/2012/03/01/install-python-2/

Run Instruction:
1. train new models:
cd into corenlp-python folder:
in one terminal tab run: corenlp/corenlp.py -S stanford-corenlp-full-2013-06-20/
in another terminal tab run: python corenlp/fextractor.py 10

To cache parser (and thus significantly speed up processing time), add an additional command line parameter telling fextractor the file in which to cache the parses, e.g.:
python corenlp/fextractor.py 10 parse.cache


